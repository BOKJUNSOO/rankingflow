{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+\n",
      "|row_number|level|         need_exp|\n",
      "+----------+-----+-----------------+\n",
      "|      NULL| NULL|             NULL|\n",
      "|         0|  260|1.731919984062E12|\n",
      "|         1|  261|1.749239183902E12|\n",
      "|         2|  262|1.766731575741E12|\n",
      "|         3|  263|1.784398891498E12|\n",
      "|         4|  264|1.802242880412E12|\n",
      "|         5|  265|2.342915744535E12|\n",
      "|         6|  266| 2.36634490198E12|\n",
      "|         7|  267|2.390008350999E12|\n",
      "|         8|  268|2.413908434508E12|\n",
      "+----------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+-----------------+\n",
      "|level|         need_exp|\n",
      "+-----+-----------------+\n",
      "|  260|1.731919984062E12|\n",
      "|  261|1.749239183902E12|\n",
      "|  262|1.766731575741E12|\n",
      "|  263|1.784398891498E12|\n",
      "|  264|1.802242880412E12|\n",
      "|  265|2.342915744535E12|\n",
      "|  266| 2.36634490198E12|\n",
      "|  267|2.390008350999E12|\n",
      "|  268|2.413908434508E12|\n",
      "|  269|2.438047518853E12|\n",
      "+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## USER 테이블 build\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "                .master(\"local\")\n",
    "                .appName(\"World Count\")\n",
    "                .getOrCreate()\n",
    ")\n",
    "file_path = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-11.json\"\n",
    "file_path1 = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-10.json\"\n",
    "df = spark.read.format('json').option(\"multiLine\", True).load(file_path)\n",
    "df1 = spark.read.format('json').option(\"multiLine\", True).load(file_path1)\n",
    "# 직업명을 제대로 채우기\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType , DoubleType\n",
    "df = df.select(F.explode(\"ranking\").alias(\"USER\"))\n",
    "df = df.select(\"USER.character_name\",\n",
    "               \"USER.date\",\n",
    "               \"USER.class_name\",\n",
    "               \"USER.sub_class_name\",\n",
    "               \"USER.character_level\",\n",
    "               \"USER.character_exp\",\n",
    "               \"USER.ranking\")\n",
    "df = df.withColumn(\"class\",df[\"sub_class_name\"])\n",
    "df = df.withColumn(\"class\",F.when(df[\"sub_class_name\"]== \"\", df[\"class_name\"]) \\\n",
    "                            .otherwise(df[\"class\"]))\n",
    "df = df.drop(\"class_name\",\"sub_class_name\")\n",
    "\n",
    "\n",
    "# 각 유저의 위치한 지역 컬럼추가\n",
    "df = df.withColumn(\"status\",\n",
    "                   F.when(df[\"character_level\"]>=290,\"Tallahart\") \\\n",
    "                    .when((df[\"character_level\"]<=289)&(df[\"character_level\"]>=285),\"Carcion\") \\\n",
    "                    .when((df[\"character_level\"]<=284)&(df[\"character_level\"]>=280),\"Arteria\") \\\n",
    "                    .when((df[\"character_level\"]<=279)&(df[\"character_level\"]>=275),\"Dowonkyung\") \\\n",
    "                    .when((df[\"character_level\"]<=274)&(df[\"character_level\"]>=270),\"Odium\") \\\n",
    "                    .when((df[\"character_level\"]<=269)&(df[\"character_level\"]>=265),\"HotelArcs\") \\\n",
    "                    .when((df[\"character_level\"]<=264)&(df[\"character_level\"]>=260),\"Cernium\") \\\n",
    "                    .otherwise(\"AcaneRiver\"))\n",
    "\n",
    "file_path = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\rankingflow\\data\\maple_exp.csv\"\n",
    "schema = StructType([\n",
    "    StructField(\"row_number\",IntegerType(),True),\n",
    "    StructField(\"level\",IntegerType(),True),\n",
    "    StructField(\"need_exp\",DoubleType(), True)\n",
    "])\n",
    "exp_df = spark.read.format('csv').schema(schema).option(\"multiLine\", True).load(f\"{file_path}\")\n",
    "exp_df.show(10)\n",
    "exp_df = exp_df.dropna()\n",
    "exp_df = exp_df.select(\"level\",\"need_exp\")\n",
    "exp_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_status = df.groupBy(\"class\",\"date\").pivot(\"status\").agg(F.count(\"status\"))\n",
    "class_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark_common\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_user_dataframe\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      4\u001b[0m     SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      5\u001b[0m                 \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m                 \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorld Count\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m                 \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spark_common'"
     ]
    }
   ],
   "source": [
    "from jobs.spark_common.base import make_user_dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "                .master(\"local\")\n",
    "                .appName(\"World Count\")\n",
    "                .getOrCreate()\n",
    ")\n",
    "file_path = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-11.json\"\n",
    "file_path1 = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-10.json\"\n",
    "df1 = make_user_dataframe(spark,file_path)\n",
    "df2 = make_user_dataframe(spark,file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\plugins',\n",
      " 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Temp\\\\spark-623e3e83-66a3-40cc-872d-2099bc8fc0ea\\\\userFiles-6a49d165-e70b-46a2-b010-e3ce9569be91',\n",
      " 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\python312.zip',\n",
      " 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\DLLs',\n",
      " 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib',\n",
      " 'C:\\\\Users\\\\brian\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312',\n",
      " 'c:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\venv',\n",
      " '',\n",
      " 'c:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\venv\\\\Lib\\\\site-packages',\n",
      " 'c:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\venv\\\\Lib\\\\site-packages\\\\win32',\n",
      " 'c:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\venv\\\\Lib\\\\site-packages\\\\win32\\\\lib',\n",
      " 'c:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\\\\venv\\\\Lib\\\\site-packages\\\\Pythonwin',\n",
      " 'C:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow',\n",
      " 'C:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow']\n",
      "C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-11.json data를 load 합니다.\n",
      "C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-10.json data를 load 합니다.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import sys\n",
    "(sys.path.append(\"C:\\\\Users\\\\brian\\\\Desktop\\\\JUNSOO\\\\rankingflow\"))\n",
    "pprint(sys.path)\n",
    "\n",
    "from jobs.spark_common.base import make_user_dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder \\\n",
    "                .master(\"local\")\n",
    "                .appName(\"World Count\")\n",
    "                .getOrCreate()\n",
    ")\n",
    "file_path = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-11.json\"\n",
    "file_path1 = r\"C:\\Users\\brian\\Desktop\\JUNSOO\\Project\\ETL_DATA\\data\\ranking_2024-10-10.json\"\n",
    "df1 = make_user_dataframe(spark,file_path)\n",
    "df2 = make_user_dataframe(spark,file_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "|character_name|      date|character_level|  character_exp|ranking|     class|   status|\n",
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "|          버터|2024-10-11|            297|843016259192919|      1|나이트로드|Tallahart|\n",
      "|          헨쇼|2024-10-11|            296|759870313066254|      2|      아델|Tallahart|\n",
      "|   케인WWE챔프|2024-10-11|            296| 24509577197550|      3|배틀메이지|Tallahart|\n",
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "|character_name|      date|character_level|  character_exp|ranking|     class|   status|\n",
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "|          버터|2024-10-10|            297|813645542531833|      1|나이트로드|Tallahart|\n",
      "|          헨쇼|2024-10-10|            296|747242572907010|      2|      아델|Tallahart|\n",
      "|   케인WWE챔프|2024-10-10|            296| 16689886609018|      3|배틀메이지|Tallahart|\n",
      "+--------------+----------+---------------+---------------+-------+----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(3)\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column character_name#1504 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m joined_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacter_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacter_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m joined_df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#df1.show(3)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#df2.show(3)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\brian\\Desktop\\JUNSOO\\rankingflow\\venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:2493\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m   2491\u001b[0m         on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jseq([])\n\u001b[0;32m   2492\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(how, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow should be a string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2493\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\brian\\Desktop\\JUNSOO\\rankingflow\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\brian\\Desktop\\JUNSOO\\rankingflow\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column character_name#1504 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
     ]
    }
   ],
   "source": [
    "joined_df = df1.join(df1,df1[\"character_name\"] == df2[\"character_name\"],how=\"inner\")\n",
    "joined_df.count()\n",
    "df1.show(3)\n",
    "df2.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
